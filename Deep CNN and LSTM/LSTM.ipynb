{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbuHYkrWDFDT"
   },
   "source": [
    "## Name: Rohit Kulkarni USC ID: 5402749044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b) & 1.c.i) & 1.c.ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMsc2yKuDFDY"
   },
   "outputs": [],
   "source": [
    "text_data = open('Concatenated input file.txt',encoding='utf-8').read()\n",
    "text_data = text_data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi0HiMFpDFDZ"
   },
   "outputs": [],
   "source": [
    "characters = sorted(list(set(text_data)))\n",
    "int_char = dict((c, i) for i, c in enumerate(characters))\n",
    "characters_number = len(text_data)\n",
    "vocab_number = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "_5YY3Jg3DFDb",
    "outputId": "f7a93d3c-bf13-47bc-808f-f3ac0e35ca6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters 1611875\n",
      "Number of vocabs 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters\",characters_number)\n",
    "print(\"Number of vocabs\",vocab_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c.iii) & 1.c.iv) & 1.c.v) & 1.c.vi) & 1.c.vii) & 1.c.viii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kd2Y9wg3DFDd"
   },
   "outputs": [],
   "source": [
    "sequence_window = 100\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, characters_number - sequence_window, 1):\n",
    "    seq_in = text_data[i:i + sequence_window]\n",
    "    seq_out = text_data[i + sequence_window]\n",
    "    X.append([int_char[char] for char in seq_in])\n",
    "    Y.append(int_char[seq_out])\n",
    "number_of_patterns = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXSBc2VbDFDf"
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(Y)\n",
    "X = numpy.reshape(X, (number_of_patterns, sequence_window, 1))\n",
    "X = X / float(vocab_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c.ix) & 1.c.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngBUAs8iDFDh"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1042
    },
    "colab_type": "code",
    "id": "IBJuO3zGDFDj",
    "outputId": "dfa49c64-fa33-4a9b-f700-636cc9eaeb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1611775/1611775 [==============================] - 190s 118us/step - loss: 3.0529\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.05289, saving model to weights-improvement-01-3.0529.hdf5\n",
      "Epoch 2/15\n",
      "1611775/1611775 [==============================] - 193s 120us/step - loss: 2.9087\n",
      "\n",
      "Epoch 00002: loss improved from 3.05289 to 2.90871, saving model to weights-improvement-02-2.9087.hdf5\n",
      "Epoch 3/15\n",
      "1611775/1611775 [==============================] - 195s 121us/step - loss: 2.8390\n",
      "\n",
      "Epoch 00003: loss improved from 2.90871 to 2.83902, saving model to weights-improvement-03-2.8390.hdf5\n",
      "Epoch 4/15\n",
      "1611775/1611775 [==============================] - 195s 121us/step - loss: 2.8101\n",
      "\n",
      "Epoch 00004: loss improved from 2.83902 to 2.81006, saving model to weights-improvement-04-2.8101.hdf5\n",
      "Epoch 5/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.7760\n",
      "\n",
      "Epoch 00005: loss improved from 2.81006 to 2.77605, saving model to weights-improvement-05-2.7760.hdf5\n",
      "Epoch 6/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.7448\n",
      "\n",
      "Epoch 00006: loss improved from 2.77605 to 2.74484, saving model to weights-improvement-06-2.7448.hdf5\n",
      "Epoch 7/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.7189\n",
      "\n",
      "Epoch 00007: loss improved from 2.74484 to 2.71887, saving model to weights-improvement-07-2.7189.hdf5\n",
      "Epoch 8/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.6932\n",
      "\n",
      "Epoch 00008: loss improved from 2.71887 to 2.69315, saving model to weights-improvement-08-2.6932.hdf5\n",
      "Epoch 9/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.6688\n",
      "\n",
      "Epoch 00009: loss improved from 2.69315 to 2.66876, saving model to weights-improvement-09-2.6688.hdf5\n",
      "Epoch 10/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.6462\n",
      "\n",
      "Epoch 00010: loss improved from 2.66876 to 2.64618, saving model to weights-improvement-10-2.6462.hdf5\n",
      "Epoch 11/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.6233\n",
      "\n",
      "Epoch 00011: loss improved from 2.64618 to 2.62325, saving model to weights-improvement-11-2.6233.hdf5\n",
      "Epoch 12/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.6018\n",
      "\n",
      "Epoch 00012: loss improved from 2.62325 to 2.60182, saving model to weights-improvement-12-2.6018.hdf5\n",
      "Epoch 13/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.5803\n",
      "\n",
      "Epoch 00013: loss improved from 2.60182 to 2.58027, saving model to weights-improvement-13-2.5803.hdf5\n",
      "Epoch 14/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.5593\n",
      "\n",
      "Epoch 00014: loss improved from 2.58027 to 2.55929, saving model to weights-improvement-14-2.5593.hdf5\n",
      "Epoch 15/15\n",
      "1611775/1611775 [==============================] - 194s 120us/step - loss: 2.5394\n",
      "\n",
      "Epoch 00015: loss improved from 2.55929 to 2.53944, saving model to weights-improvement-15-2.5394.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f757f14cf28>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=15, batch_size=4096, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c.xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7h34uBYDFDl"
   },
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-15-2.5394.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "int_to_char = dict((i, c) for i, c in enumerate(characters))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "zVwAYXNwDFDn",
    "outputId": "5746cb4c-7b85-4774-b91a-13afe1793543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" there are those who take mental phenomena naively, just as they would physical phenomena. this schoo \"\n",
      "t oh the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soele tf the soe"
     ]
    }
   ],
   "source": [
    "p = \"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\".lower()\n",
    "pattern = [char_to_int[char] for char in p]\n",
    "pattern = pattern[0:100]\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_number)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    result = int_to_char[index]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
